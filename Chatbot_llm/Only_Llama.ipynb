{"cells":[{"cell_type":"markdown","metadata":{"id":"4bKQIsIq-d8y"},"source":["#**Llama 2**"]},{"cell_type":"markdown","metadata":{"id":"PnV5UC7A2vBZ"},"source":["The Llama 2 is a collection of pretrained and fine-tuned generative text models, ranging from 7 billion to 70 billion parameters, designed for dialogue use cases.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AC41zK5l3Abp"},"source":[" It outperforms open-source chat models on most benchmarks and is on par with popular closed-source models in human evaluations for helpfulness and safety."]},{"cell_type":"markdown","metadata":{"id":"4nobX9E83PjQ"},"source":["[Llama 2 13B-chat](https://huggingface.co/meta-llama/Llama-2-13b-chat)"]},{"cell_type":"markdown","metadata":{"id":"0K4QuEDH4CbY"},"source":["`llama.cpp`'s objective is to run the LLaMA model with 4-bit integer quantization on MacBook. It is a plain C/C++ implementation optimized for Apple silicon and x86 architectures, supporting various integer quantization and BLAS libraries. Originally a web chat example, it now serves as a development playground for ggml library features.\n","\n","`GGML`, a C library for machine learning, facilitates the distribution of large language models (LLMs). It utilizes quantization to enable efficient LLM execution on consumer hardware. GGML files contain binary-encoded data, including version number, hyperparameters, vocabulary, and weights. The vocabulary comprises tokens for language generation, while the weights determine the LLM's size. Quantization reduces precision to optimize resource usage."]},{"cell_type":"markdown","metadata":{"id":"3YC846SH5DOK"},"source":["#  Quantized Models from the Hugging Face Community"]},{"cell_type":"markdown","metadata":{"id":"0TD82wis5LGA"},"source":["The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n","\n","There are several variations available, but the ones that interest us are based on the GGLM library.\n","\n","We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n","\n","\n","\n","In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."]},{"cell_type":"markdown","metadata":{"id":"YQZBmz7I5neU"},"source":["#**Step 1: Install All the Required Packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L0avf7xx2lcj","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1716210772943,"user_tz":-120,"elapsed":92157,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"4be5f663-bc20-4688-ffaa-28a024d58612"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","Collecting llama-cpp-python==0.1.78\n","  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Running command pip subprocess to install build dependencies\n","  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n","  Collecting setuptools>=42\n","    Using cached setuptools-69.5.1-py3-none-any.whl (894 kB)\n","  Collecting scikit-build>=0.13\n","    Using cached scikit_build-0.17.6-py3-none-any.whl (84 kB)\n","  Collecting cmake>=3.18\n","    Using cached cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","  Collecting ninja\n","    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","  Collecting distro (from scikit-build>=0.13)\n","    Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n","  Collecting packaging (from scikit-build>=0.13)\n","    Using cached packaging-24.0-py3-none-any.whl (53 kB)\n","  Collecting tomli (from scikit-build>=0.13)\n","    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n","  Collecting wheel>=0.32.0 (from scikit-build>=0.13)\n","    Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n","  Installing collected packages: ninja, wheel, tomli, setuptools, packaging, distro, cmake, scikit-build\n","    Creating /tmp/pip-build-env-qp6t3el7/overlay/local/bin\n","    changing mode of /tmp/pip-build-env-qp6t3el7/overlay/local/bin/ninja to 755\n","    changing mode of /tmp/pip-build-env-qp6t3el7/overlay/local/bin/wheel to 755\n","    changing mode of /tmp/pip-build-env-qp6t3el7/overlay/local/bin/distro to 755\n","    changing mode of /tmp/pip-build-env-qp6t3el7/overlay/local/bin/cmake to 755\n","    changing mode of /tmp/pip-build-env-qp6t3el7/overlay/local/bin/cpack to 755\n","    changing mode of /tmp/pip-build-env-qp6t3el7/overlay/local/bin/ctest to 755\n","  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","  tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n","  Successfully installed cmake-3.29.3 distro-1.9.0 ninja-1.11.1.1 packaging-24.0 scikit-build-0.17.6 setuptools-69.5.1 tomli-2.0.1 wheel-0.43.0\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Running command Getting requirements to build wheel\n","  running egg_info\n","  writing llama_cpp_python.egg-info/PKG-INFO\n","  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n","  writing requirements to llama_cpp_python.egg-info/requires.txt\n","  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n","  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE.md'\n","  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Running command Preparing metadata (pyproject.toml)\n","  running dist_info\n","  creating /tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info\n","  writing /tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/PKG-INFO\n","  writing dependency_links to /tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/dependency_links.txt\n","  writing requirements to /tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/requires.txt\n","  writing top-level names to /tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/top_level.txt\n","  writing manifest file '/tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/SOURCES.txt'\n","  reading manifest file '/tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE.md'\n","  writing manifest file '/tmp/pip-modern-metadata-schpv63w/llama_cpp_python.egg-info/SOURCES.txt'\n","  creating '/tmp/pip-modern-metadata-schpv63w/llama_cpp_python-0.1.78.dist-info'\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy==1.23.4\n","  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m150.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.78)\n","  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m232.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Running command Building wheel for llama-cpp-python (pyproject.toml)\n","\n","\n","  --------------------------------------------------------------------------------\n","  -- Trying 'Ninja' generator\n","  --------------------------------\n","  ---------------------------\n","  ----------------------\n","  -----------------\n","  ------------\n","  -------\n","  --\n","  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n","    Compatibility with CMake < 3.5 will be removed from a future version of\n","    CMake.\n","\n","    Update the VERSION argument <min> value or use a ...<max> suffix to tell\n","    CMake that the project does not need compatibility with older versions.\n","\n","  Not searching for unused variables given on the command line.\n","\n","  -- The C compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Configuring done (0.3s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_cmake_test_compile/build\n","  --\n","  -------\n","  ------------\n","  -----------------\n","  ----------------------\n","  ---------------------------\n","  --------------------------------\n","  -- Trying 'Ninja' generator - success\n","  --------------------------------------------------------------------------------\n","\n","  Configuring Project\n","    Working directory:\n","      /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-build\n","    Command:\n","      /tmp/pip-build-env-qp6t3el7/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-qp6t3el7/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-qp6t3el7/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-qp6t3el7/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on\n","\n","  Not searching for unused variables given on the command line.\n","  -- The C compiler identification is GNU 11.4.0\n","  -- The CXX compiler identification is GNU 11.4.0\n","  -- Detecting C compiler ABI info\n","  -- Detecting C compiler ABI info - done\n","  -- Check for working C compiler: /usr/bin/cc - skipped\n","  -- Detecting C compile features\n","  -- Detecting C compile features - done\n","  -- Detecting CXX compiler ABI info\n","  -- Detecting CXX compiler ABI info - done\n","  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n","  -- Detecting CXX compile features\n","  -- Detecting CXX compile features - done\n","  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n","  fatal: not a git repository (or any of the parent directories): .git\n","  fatal: not a git repository (or any of the parent directories): .git\n","  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):\n","    Git repository not found; to enable automatic generation of build info,\n","    make sure Git is installed and the project is a Git repository.\n","\n","\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n","  -- Found Threads: TRUE\n","  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.2.140\")\n","  -- cuBLAS found\n","  -- The CUDA compiler identification is NVIDIA 12.2.140\n","  -- Detecting CUDA compiler ABI info\n","  -- Detecting CUDA compiler ABI info - done\n","  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n","  -- Detecting CUDA compile features\n","  -- Detecting CUDA compile features - done\n","  -- Using CUDA architectures: 52;61;70\n","  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n","  -- x86 detected\n","  -- Configuring done (2.7s)\n","  -- Generating done (0.0s)\n","  -- Build files have been written to: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-build\n","  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o\n","  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o\n","  [3/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o\n","  [4/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o\n","  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n","  [6/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a\n","  [7/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so\n","  [8/9] Linking CXX shared library vendor/llama.cpp/libllama.so\n","  [8/9] Install the project...\n","  -- Install configuration: \"Release\"\n","  -- Installing: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so\n","  -- Installing: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so\" to \"\"\n","  -- Installing: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py\n","  -- Installing: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py\n","  -- Installing: /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\n","  -- Set non-toolchain portion of runtime path of \"/tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so\" to \"\"\n","\n","  copying llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py\n","  copying llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py\n","  copying llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py\n","  copying llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py\n","  copying llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py\n","  copying llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py\n","  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server\n","  copying llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py\n","  copying llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py\n","  copying llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py\n","  copying /tmp/pip-install-12ggcha9/llama-cpp-python_d3b97626e5684875b6428a96f50bc8ba/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed\n","\n","  running bdist_wheel\n","  running build\n","  running build_py\n","  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310\n","  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server\n","  copied 9 files\n","  running build_ext\n","  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n","  running install\n","  running install_lib\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp\n","  copied 11 files\n","  running install_data\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n","  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -> _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin\n","  running install_egg_info\n","  running egg_info\n","  writing llama_cpp_python.egg-info/PKG-INFO\n","  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt\n","  writing requirements to llama_cpp_python.egg-info/requires.txt\n","  writing top-level names to llama_cpp_python.egg-info/top_level.txt\n","  reading manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  adding license file 'LICENSE.md'\n","  writing manifest file 'llama_cpp_python.egg-info/SOURCES.txt'\n","  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info\n","  running install_scripts\n","  copied 0 files\n","  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL\n","  creating '/tmp/pip-wheel-jqlm5a26/.tmp-ofth3mqz/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl' and adding '_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel' to it\n","  adding 'llama_cpp/__init__.py'\n","  adding 'llama_cpp/libllama.so'\n","  adding 'llama_cpp/llama.py'\n","  adding 'llama_cpp/llama_cpp.py'\n","  adding 'llama_cpp/llama_grammar.py'\n","  adding 'llama_cpp/llama_types.py'\n","  adding 'llama_cpp/py.typed'\n","  adding 'llama_cpp/utils.py'\n","  adding 'llama_cpp/server/__init__.py'\n","  adding 'llama_cpp/server/__main__.py'\n","  adding 'llama_cpp/server/app.py'\n","  adding 'llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py'\n","  adding 'llama_cpp_python-0.1.78.data/data/bin/convert.py'\n","  adding 'llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so'\n","  adding 'llama_cpp_python-0.1.78.data/data/lib/libllama.so'\n","  adding 'llama_cpp_python-0.1.78.dist-info/LICENSE.md'\n","  adding 'llama_cpp_python-0.1.78.dist-info/METADATA'\n","  adding 'llama_cpp_python-0.1.78.dist-info/WHEEL'\n","  adding 'llama_cpp_python-0.1.78.dist-info/top_level.txt'\n","  adding 'llama_cpp_python-0.1.78.dist-info/RECORD'\n","  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811143 sha256=5bcca598776625d70f9a1da81ec8eb2781d4f906eb1cbdd7045913e441dfc3cd\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-dn_cwzu2/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.11.0\n","    Uninstalling typing_extensions-4.11.0:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.11.0.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n","      Successfully uninstalled typing_extensions-4.11.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.4\n","    Uninstalling numpy-1.23.4:\n","      Removing file or directory /usr/local/bin/f2py\n","      Removing file or directory /usr/local/bin/f2py3\n","      Removing file or directory /usr/local/bin/f2py3.10\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.4.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n","      Successfully uninstalled numpy-1.23.4\n","  changing mode of /usr/local/bin/f2py to 755\n","  changing mode of /usr/local/bin/f2py3 to 755\n","  changing mode of /usr/local/bin/f2py3.10 to 755\n","  Attempting uninstall: diskcache\n","    Found existing installation: diskcache 5.6.3\n","    Uninstalling diskcache-5.6.3:\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n","      Successfully uninstalled diskcache-5.6.3\n","  Attempting uninstall: llama-cpp-python\n","    Found existing installation: llama_cpp_python 0.1.78\n","    Uninstalling llama_cpp_python-0.1.78:\n","      Removing file or directory /usr/local/bin/__pycache__/convert-lora-to-ggml.cpython-310.pyc\n","      Removing file or directory /usr/local/bin/__pycache__/convert.cpython-310.pyc\n","      Removing file or directory /usr/local/bin/convert-lora-to-ggml.py\n","      Removing file or directory /usr/local/bin/convert.py\n","      Removing file or directory /usr/local/lib/libggml_shared.so\n","      Removing file or directory /usr/local/lib/libllama.so\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n","      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.1.78.dist-info/\n","      Successfully uninstalled llama_cpp_python-0.1.78\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\n","pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.4 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.11.0\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n","Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.4)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n","Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.10/dist-packages (1.23.4)\n"]}],"source":["# # GPU llama-cpp-python\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose\n","!pip install huggingface_hub\n","!pip install llama-cpp-python==0.1.78\n","!pip install numpy==1.23.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJ90LnMv54Y-"},"outputs":[],"source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format"]},{"cell_type":"markdown","metadata":{"id":"6lOmpKB36RJh"},"source":["#**Step 2: Import All the Required Libraries**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ak3ZtGjM6Wdp"},"outputs":[],"source":["from huggingface_hub import hf_hub_download"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85XOzmui6rGN"},"outputs":[],"source":["from llama_cpp import Llama"]},{"cell_type":"markdown","metadata":{"id":"haAb9kNm6J9n"},"source":["#**Step 3: Download the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBgdGV4b6MxG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7db19b8-b596-4be4-c09f-a013fdf5ccab","executionInfo":{"status":"ok","timestamp":1716210800070,"user_tz":-120,"elapsed":1903,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"]},{"cell_type":"markdown","metadata":{"id":"VQ6OYnI46kKq"},"source":["#**Step 4: Loading the Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irftToUj6aWt","colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"bcc0c328-ae6e-4b8c-dda8-3068e1ddf941","executionInfo":{"status":"error","timestamp":1716210674018,"user_tz":-120,"elapsed":978,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Llama' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b5bee35a0431>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlcpp_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m lcpp_llm = Llama(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# CPU cores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Llama' is not defined"]}],"source":["# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=3000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YG4Pylz662At","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aa8eadb2-d23d-4f86-b1d8-f9449196cf15","executionInfo":{"status":"ok","timestamp":1716210592174,"user_tz":-120,"elapsed":319,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["32"]},"metadata":{},"execution_count":9}],"source":["# See the number of layers in GPU\n","lcpp_llm.params.n_gpu_layers"]},{"cell_type":"markdown","metadata":{"id":"iE-M307R6_pT"},"source":["#**Step 5: Create a Prompt Template**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfzwELMC7Dyg"},"outputs":[],"source":["prompt = \"What is universal expo Paris?\""]},{"cell_type":"markdown","metadata":{"id":"aT8pg6zt7QzA"},"source":["#**Step 6: Generating the Response**"]},{"cell_type":"markdown","source":["### Default trial"],"metadata":{"id":"62ySTql3Iv98"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0aF0qWUJ7OPK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716208083524,"user_tz":-120,"elapsed":59436,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"cf721235-da05-4f45-be02-308dfd1c6e03"},"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]}],"source":["response=lcpp_llm(prompt=prompt, max_tokens=512, temperature=0.8, top_p=0.95,\n","                  repeat_penalty=1.2, top_k=150,\n","                  echo=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qona58gX8oAn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"145ed48c-d679-45f2-d329-0f3340d1ad87","executionInfo":{"status":"ok","timestamp":1716208114865,"user_tz":-120,"elapsed":335,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["What is universal expo Paris?\n","The Universal Exposition of 1867, also known as the Exposition Universelle, was a world's fair held in Paris, France, from April 25 to November 3, 1867. It was the second industrial exposition ever held and it aimed to showcase the latest technological advancements and cultural achievements of nations around the world.\n","The expo featured a vast array of exhibits, including machinery, textiles, metallurgy, mining, agriculture, and the arts. It also included a section dedicated to the colonies and territories of France, showcasing their natural resources, industries, and cultures.\n","One of the most notable features of the expo was the Crystal Palace, a massive glass and iron structure that housed many of the exhibits. The palace was designed by British architect Joseph Paxton and it quickly became an iconic symbol of the fair.\n","The Universal Exposition of 1867 in Paris marked an important milestone in the history of world's fairs, as it set the standard for future expositions and paved the way for the modern international exhibitions that we know today.\n"]}],"source":["print(response[\"choices\"][0][\"text\"])"]},{"cell_type":"markdown","source":["### Default trial 2"],"metadata":{"id":"KRyLYnPwI0Xh"}},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4\n","!pip install huggingface_hub\n","!pip install llama-cpp-python==0.1.78\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"yXFqjoKdNOm5","executionInfo":{"status":"ok","timestamp":1716195613416,"user_tz":-120,"elapsed":89174,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"ec21c279-086d-41b1-d7c8-4986ddfb0955"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python==0.1.78\n","  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting numpy==1.23.4\n","  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.78)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5811061 sha256=4e0b85c9349fe12ffac5f7c937d3bb619086caf69a4b664d533f331959a89915\n","  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n","Successfully built llama-cpp-python\n","Installing collected packages: numpy, diskcache, llama-cpp-python\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.4 which is incompatible.\n","pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.4 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n","Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.11.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.4)\n","Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n"]}]},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download\n","from llama_cpp import Llama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":331},"id":"QE9hLNBUNPaY","executionInfo":{"status":"error","timestamp":1716196927156,"user_tz":-120,"elapsed":397,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"51741b98-b735-481a-d26e-9ca0b57813c7"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'llama_cpp'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-1476c069d144>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhf_hub_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n","model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n","\n","model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["5f46e7ce7ff543a8b86d186bccbb1be6","36a9597fcac54cc9aa24b8a06b807a97","4ea9322d8a0047edaad8b7f40bd46569","b39428e9896f46f991a7e24669425e6f","82edc6fffb9a42c2a30facaf16bf4dfc","d751983fb958486687c476bd025622a3","33f428a8207d4370934d302166c2ecae","9da9543019944de39331100ef340cf66","3fa674eff2da47d091b2313fa456a91f","87a68119b0c04194a567428d04c70c06","c6c53ca327e84b7e842738ea1ceb46f4"]},"id":"F7G8G289NTC9","outputId":"277432a4-c5ec-47c9-a551-0001d3953020"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["llama-2-13b-chat.ggmlv3.q5_1.bin:   0%|          | 0.00/9.76G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f46e7ce7ff543a8b86d186bccbb1be6"}},"metadata":{}}]},{"cell_type":"code","source":["lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2,  # CPU cores\n","    n_batch=1000,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32  # Change this value based on your model and your GPU VRAM pool.\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wWrLCZLNbS-","executionInfo":{"status":"ok","timestamp":1716195986108,"user_tz":-120,"elapsed":1753,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"1c382a09-02d2-4bde-df0e-c0b943e9eeaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"]}]},{"cell_type":"code","source":["# Define your paragraph\n","paragraph = \"\"\"\n","The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields. From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity. Visitors from all around the world gather to witness the marvels on display, making it a truly global affair. The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects and concepts that promise to shape the world of tomorrow.\n","\"\"\"\n","\n","# Incorporate the paragraph into the prompt\n","prompt = f\"\"\"\n","Context: {paragraph}\n","\n","Question: What is the significance of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response = lcpp_llm(prompt=prompt, max_tokens=256, temperature=1, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n","\n","# Print the generated response\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"id":"PZsY_DYdI2rS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716196025190,"user_tz":-120,"elapsed":36215,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"a42d0fa4-9b19-404f-e29d-3dcfec06e222"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Context: \n","The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields. From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity. Visitors from all around the world gather to witness the marvels on display, making it a truly global affair. The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects and concepts that promise to shape the world of tomorrow.\n","\n","\n","Question: What is the significance of the Universal Expo in Paris?\n","A) To showcase cultural exhibitions from around the world \n","B) To display technological innovations for the future \n","C) A platform for global leaders to discuss pressing issues \n","D) All of the above\n","\n","Correct answer: D) All of the above\n","\n","Explanation: The Universal Expo in Paris serves multiple purposes. It provides a platform for nations to showcase their cultural exhibitions, technological innovations, and groundbreaking projects that shape the world of tomorrow. Additionally, it offers a space for global leaders to discuss pressing issues, fostering collaboration and cooperation among nations. Therefore, options A, B, and C are all correct, making the answer D) All of the above.\n"]}]},{"cell_type":"code","source":["# Define your paragraph\n","paragraph = \"\"\"\n","Explanation of the Main Building and Grounds:\n","\n","The chief building of the exhibition covers an area of 40 acres. In exterior it resembles a huge oblong coliseum chiefly composed of metal framework interfilled with glass. Although convenient, and ingeniously designed throughout, externally it presents a massive rather than a magnificent appearance. It is surrounded by a park, studded with delightful gardens, scientifically laid out, containing a variety of exotic and other plants, and designed to unite the picturesque with the useful.(Spedon 189-191)\n","\n","\"\"\"\n","\n","# Incorporate the paragraph into the prompt\n","prompt = f\"\"\"\n","Context: {paragraph}\n","\n","Question: What is the significance of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response = lcpp_llm(prompt=prompt, max_tokens=256, temperature=1, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n","\n","# Print the generated response\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QyJFBiFmNN8c","executionInfo":{"status":"ok","timestamp":1716196274834,"user_tz":-120,"elapsed":19537,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"f797e1e0-32a2-4ac9-d1c4-491a6d8bd4dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["\n","Context: \n","Explanation of the Main Building and Grounds:\n","\n","The chief building of the exhibition covers an area of 40 acres. In exterior it resembles a huge oblong coliseum chiefly composed of metal framework interfilled with glass. Although convenient, and ingeniously designed throughout, externally it presents a massive rather than a magnificent appearance. It is surrounded by a park, studded with delightful gardens, scientifically laid out, containing a variety of exotic and other plants, and designed to unite the picturesque with the useful.(Spedon 189-191)\n","\n","\n","\n","Question: What is the significance of the Universal Expo in Paris?\n","A. It was intended as an international event bringing together all branches of science, art and industry on a glittering scale, especially showcasing French culture.\n","B. To be a spectacular celebration of modern technology and progress with innovative metal structures and glass interiors\n","C. A scientific experiment to explore the use of exotic plants in garden design. \n"]}]},{"cell_type":"code","source":["# Define your paragraphs\n","paragraphs = [\n","    \"\"\"\n","    The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields.\n","    From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity.\n","    Visitors from all around the world gather to witness the marvels on display, making it a truly global affair.\n","    The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects\n","    and concepts that promise to shape the world of tomorrow.\n","    \"\"\",\n","    \"\"\"\n","    The Universal Expo has historically been a platform where countries demonstrate their technological prowess and\n","    cultural heritage. It is a place where the future is envisioned, and the past is celebrated. The event plays a\n","    crucial role in fostering international cooperation and understanding, as nations come together to share their\n","    achievements and aspirations. Each pavilion tells a unique story, offering visitors an immersive experience into\n","    the culture and innovations of different countries.\n","    \"\"\",\n","    \"\"\"\n","    One of the key highlights of the Universal Expo is its focus on sustainability and innovation. Countries\n","    showcase their latest advancements in renewable energy, sustainable architecture, and environmental conservation.\n","    The Expo serves as a reminder of the collective responsibility we have towards our planet and inspires actions\n","    towards a more sustainable future. Through interactive exhibits and educational programs, visitors learn about\n","    the importance of sustainability and the steps being taken globally to address environmental challenges.\n","    \"\"\"\n","]\n","\n","# Combine the paragraphs into a single context string\n","context = \"\\n\".join(paragraphs)\n","\n","# Incorporate the combined context into the prompt\n","prompt = f\"\"\"\n","Context: {context}\n","\n","Question: What is the significance of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response = lcpp_llm(prompt=prompt, max_tokens=256, temperature=1, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n","\n","# Print the generated response\n","print(response[\"choices\"][0][\"text\"])\n"],"metadata":{"id":"ydiPWQTTI2uL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716196395150,"user_tz":-120,"elapsed":18312,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"60c2bb0b-6242-43bf-be18-1ce37f9e4aa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["\n","Context: \n","    The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields.\n","    From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity.\n","    Visitors from all around the world gather to witness the marvels on display, making it a truly global affair.\n","    The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects\n","    and concepts that promise to shape the world of tomorrow.\n","    \n","\n","    The Universal Expo has historically been a platform where countries demonstrate their technological prowess and\n","    cultural heritage. It is a place where the future is envisioned, and the past is celebrated. The event plays a\n","    crucial role in fostering international cooperation and understanding, as nations come together to share their\n","    achievements and aspirations. Each pavilion tells a unique story, offering visitors an immersive experience into\n","    the culture and innovations of different countries.\n","    \n","\n","    One of the key highlights of the Universal Expo is its focus on sustainability and innovation. Countries\n","    showcase their latest advancements in renewable energy, sustainable architecture, and environmental conservation.\n","    The Expo serves as a reminder of the collective responsibility we have towards our planet and inspires actions\n","    towards a more sustainable future. Through interactive exhibits and educational programs, visitors learn about\n","    the importance of sustainability and the steps being taken globally to address environmental challenges.\n","    \n","\n","Question: What is the significance of the Universal Expo in Paris?\n","      A) It provides an opportunity for countries to showcase their cultural heritage \n","       B) It allows nations to demonstrate their technological advancements and innovations  \n","      C) It fosters international cooperation and understanding through shared achievements  \n","       D) All of the above. \n"]}]},{"cell_type":"code","source":["# Define your paragraphs\n","paragraphs = [\n","    \"\"\"\n","    The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields.\n","    From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity.\n","    Visitors from all around the world gather to witness the marvels on display, making it a truly global affair.\n","    The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects\n","    and concepts that promise to shape the world of tomorrow.\n","    \"\"\",\n","    \"\"\"\n","    The Universal Expo has historically been a platform where countries demonstrate their technological prowess and\n","    cultural heritage. It is a place where the future is envisioned, and the past is celebrated. The event plays a\n","    crucial role in fostering international cooperation and understanding, as nations come together to share their\n","    achievements and aspirations. Each pavilion tells a unique story, offering visitors an immersive experience into\n","    the culture and innovations of different countries.\n","    \"\"\",\n","    \"\"\"\n","    One of the key highlights of the Universal Expo is its focus on sustainability and innovation. Countries\n","    showcase their latest advancements in renewable energy, sustainable architecture, and environmental conservation.\n","    The Expo serves as a reminder of the collective responsibility we have towards our planet and inspires actions\n","    towards a more sustainable future. Through interactive exhibits and educational programs, visitors learn about\n","    the importance of sustainability and the steps being taken globally to address environmental challenges.\n","    \"\"\"\n","]\n","\n","# Combine the paragraphs into a single context string\n","context = \"\\n\".join(paragraphs)\n","\n","# Incorporate the combined context into the prompt\n","prompt = f\"\"\"\n","Context: {context}\n","\n","Question: What is the significance of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response = lcpp_llm(prompt=prompt, max_tokens=256, temperature=0, top_p=0.95, repeat_penalty=1.2, top_k=150, echo=True)\n","\n","# Print the generated response\n","print(response[\"choices\"][0][\"text\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBawBIODRnv0","executionInfo":{"status":"ok","timestamp":1716196470381,"user_tz":-120,"elapsed":17360,"user":{"displayName":"Nil Yagmur Ilba","userId":"06892685453810341926"}},"outputId":"ac596b61-3157-4901-ef09-6a9d29e88659"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["\n","Context: \n","    The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields.\n","    From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity.\n","    Visitors from all around the world gather to witness the marvels on display, making it a truly global affair.\n","    The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects\n","    and concepts that promise to shape the world of tomorrow.\n","    \n","\n","    The Universal Expo has historically been a platform where countries demonstrate their technological prowess and\n","    cultural heritage. It is a place where the future is envisioned, and the past is celebrated. The event plays a\n","    crucial role in fostering international cooperation and understanding, as nations come together to share their\n","    achievements and aspirations. Each pavilion tells a unique story, offering visitors an immersive experience into\n","    the culture and innovations of different countries.\n","    \n","\n","    One of the key highlights of the Universal Expo is its focus on sustainability and innovation. Countries\n","    showcase their latest advancements in renewable energy, sustainable architecture, and environmental conservation.\n","    The Expo serves as a reminder of the collective responsibility we have towards our planet and inspires actions\n","    towards a more sustainable future. Through interactive exhibits and educational programs, visitors learn about\n","    the importance of sustainability and the steps being taken globally to address environmental challenges.\n","    \n","\n","Question: What is the significance of the Universal Expo in Paris?\n","\n","A) It showcases technological innovations from around the world.\n","B) It celebrates cultural heritage and traditions.\n","C) It fosters international cooperation and understanding.\n","D) It focuses on sustainability and environmental conservation.\n","E) All of the above.\n","\n","Correct answer: E) All of the above.\n"]}]},{"cell_type":"code","source":["##"],"metadata":{"id":"leNv5Tx9Rz2y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Trial 3"],"metadata":{"id":"zRHNitW7Tbxd"}},{"cell_type":"code","source":["# Define your paragraphs\n","paragraphs = [\n","    \"\"\"\n","    The Universal Expo in Paris is a grand event that showcases the achievements of nations across various fields.\n","    From technological innovations to cultural exhibitions, the event is a melting pot of ideas and creativity.\n","    Visitors from all around the world gather to witness the marvels on display, making it a truly global affair.\n","    The Expo is not just about the present but also offers a glimpse into the future, with groundbreaking projects\n","    and concepts that promise to shape the world of tomorrow.\n","    \"\"\",\n","    \"\"\"\n","    The Universal Expo has historically been a platform where countries demonstrate their technological prowess and\n","    cultural heritage. It is a place where the future is envisioned, and the past is celebrated. The event plays a\n","    crucial role in fostering international cooperation and understanding, as nations come together to share their\n","    achievements and aspirations. Each pavilion tells a unique story, offering visitors an immersive experience into\n","    the culture and innovations of different countries.\n","    \"\"\",\n","    \"\"\"\n","    One of the key highlights of the Universal Expo is its focus on sustainability and innovation. Countries\n","    showcase their latest advancements in renewable energy, sustainable architecture, and environmental conservation.\n","    The Expo serves as a reminder of the collective responsibility we have towards our planet and inspires actions\n","    towards a more sustainable future. Through interactive exhibits and educational programs, visitors learn about\n","    the importance of sustainability and the steps being taken globally to address environmental challenges.\n","    \"\"\"\n","]\n","\n","# Combine the paragraphs into a single training data string\n","training_data = \"\\n\".join(paragraphs)\n"],"metadata":{"id":"fuiGIHhgTdUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=3000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n","    )\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4zXtmyGTexE","executionInfo":{"status":"ok","timestamp":1716208387841,"user_tz":-120,"elapsed":4052,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"a3956c31-349e-4d4f-a090-2a2fa1332c59"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"]}]},{"cell_type":"code","source":["# Combine the training data and the question\n","final_prompt = f\"\"\"\n","{training_data}\n","\n","Given the information above, what is the significance of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response = lcpp_llm(prompt=final_prompt, max_tokens=1024, temperature=0.8, top_p=0.95, repeat_penalty=1.2, top_k=150)\n","\n","# Print the generated response\n","print(response[\"choices\"][0][\"text\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UvbQCGjrThM8","executionInfo":{"status":"ok","timestamp":1716208809742,"user_tz":-120,"elapsed":16559,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"c32ea3bd-b892-47b1-f70a-45d9c09c500a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["\n","A) It showcases the achievements of nations across various fields.\n","B) It offers a glimpse into the future through groundbreaking projects and concepts.\n","C) It fosters international cooperation and understanding.\n","D) It highlights sustainability and innovation through exhibits and educational programs.\n"]}]},{"cell_type":"code","source":["# Define your paragraphs\n","paragraphs_2 = [\n","    \"\"\"\n","    The chief building of the exhibition covers an area of 40 acres. In exterior it resembles a huge oblong coliseum chiefly composed of metal framework interfilled with glass. Although convenient, and ingeniously designed throughout, externally it presents a massive rather than a magnificent appearance. It is surrounded by a park, studded with delightful gardens, scientifically laid out, containing a variety of exotic and other plants, and designed to unite the picturesque with the useful. In the park were also domestic animals of various species, implements and products of husbandry, &c., &c., displaying the triumphs of the agriculturist, with illustrations of the methods by which those results have been obtained. There were also models of improved houses, churches, furniture; houses and palaces illustrative of the manners, customs, &c., of the civilized nations of the East; among which were, mosque of the Sultan, temple of Mariette Bey, Russian village by Russian carpenters, a joint production of 150 different manufacturers; annexed were stabling and coach-houses containing horses and carriages of Russia.\n","    \"\"\"\n","]\n","\n","# Combine the paragraphs into a single training data string\n","training_data_2 = \"\\n\".join(paragraphs_2)\n","\n","# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=3000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n","    )\n","\n","\n","\n","# Combine the training data and the question\n","final_prompt_2 = f\"\"\"\n","{training_data_2}\n","\n","Given the information above, what is the geographical structure of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response_2 = lcpp_llm(prompt=final_prompt_2, max_tokens=512, temperature=0.8, top_p=0.95, repeat_penalty=1.2, top_k=150)\n","\n","# Print the generated response\n","print(response_2[\"choices\"][0][\"text\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"JYSacUAVFiXk","executionInfo":{"status":"error","timestamp":1716210665460,"user_tz":-120,"elapsed":607,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"4a58442b-bc4b-4796-b2da-4b56c0ff9cc9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'Llama' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5b25e41009f2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlcpp_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m lcpp_llm = Llama(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# CPU cores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Llama' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bvI4rg15UwYK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Trial 4"],"metadata":{"id":"qHF2A8ewK0xG"}},{"cell_type":"code","source":[],"metadata":{"id":"Ke40WsqpK2Rk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define your paragraphs\n","paragraphs = [\n","    \"\"\"\n","There are 15 entrances to the grounds of the building, and 16 doors to the building itself.\n","The French and English departments are by far the largest. The design throughout is ingenious and economical,\n","as it combines convenience, regularity and accommodation in as small a space as possible,\n","the only deficiency being in a lack of a prospective or comprehensive view. (Spedon 191)\n","\n","    \"\"\",\n","    \"\"\"\n","\n","    \"\"\",\n","    \"\"\"\n","\n","    \"\"\"\n","]\n","\n","# Combine the paragraphs into a single training data string\n","training_data = \"\\n\".join(paragraphs)\n"],"metadata":{"id":"RztIzs_JK2j4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# GPU\n","lcpp_llm = None\n","lcpp_llm = Llama(\n","    model_path=model_path,\n","    n_threads=2, # CPU cores\n","    n_batch=3000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n","    n_gpu_layers=32 # Change this value based on your model and your GPU VRAM pool.\n","    )\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716208387841,"user_tz":-120,"elapsed":4052,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"a3956c31-349e-4d4f-a090-2a2fa1332c59","id":"4HAiv3ExK2j5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"]}]},{"cell_type":"code","source":["# Combine the training data and the question\n","final_prompt = f\"\"\"\n","{training_data}\n","\n","Given the information above, what is the significance of the Universal Expo in Paris?\n","\"\"\"\n","\n","# Set up parameters for response generation\n","response = lcpp_llm(prompt=final_prompt, max_tokens=1024, temperature=0.8, top_p=0.95, repeat_penalty=1.2, top_k=150)\n","\n","# Print the generated response\n","print(response[\"choices\"][0][\"text\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716208809742,"user_tz":-120,"elapsed":16559,"user":{"displayName":"Jiaming Jiang","userId":"08696104848346964494"}},"outputId":"c32ea3bd-b892-47b1-f70a-45d9c09c500a","id":"xQuGT39eK2j5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: prefix-match hit\n"]},{"output_type":"stream","name":"stdout","text":["\n","A) It showcases the achievements of nations across various fields.\n","B) It offers a glimpse into the future through groundbreaking projects and concepts.\n","C) It fosters international cooperation and understanding.\n","D) It highlights sustainability and innovation through exhibits and educational programs.\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"137eawajh2IzUOWm6S2QWO7EdDSo81ca-","timestamp":1714574373669}],"gpuClass":"premium","collapsed_sections":["4bKQIsIq-d8y","VQ6OYnI46kKq","iE-M307R6_pT"],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5f46e7ce7ff543a8b86d186bccbb1be6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_36a9597fcac54cc9aa24b8a06b807a97","IPY_MODEL_4ea9322d8a0047edaad8b7f40bd46569","IPY_MODEL_b39428e9896f46f991a7e24669425e6f"],"layout":"IPY_MODEL_82edc6fffb9a42c2a30facaf16bf4dfc"}},"36a9597fcac54cc9aa24b8a06b807a97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d751983fb958486687c476bd025622a3","placeholder":"​","style":"IPY_MODEL_33f428a8207d4370934d302166c2ecae","value":"llama-2-13b-chat.ggmlv3.q5_1.bin:  66%"}},"4ea9322d8a0047edaad8b7f40bd46569":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_9da9543019944de39331100ef340cf66","max":9763701888,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fa674eff2da47d091b2313fa456a91f","value":6490685440}},"b39428e9896f46f991a7e24669425e6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87a68119b0c04194a567428d04c70c06","placeholder":"​","style":"IPY_MODEL_c6c53ca327e84b7e842738ea1ceb46f4","value":" 6.49G/9.76G [01:40&lt;00:45, 72.0MB/s]"}},"82edc6fffb9a42c2a30facaf16bf4dfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d751983fb958486687c476bd025622a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33f428a8207d4370934d302166c2ecae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9da9543019944de39331100ef340cf66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa674eff2da47d091b2313fa456a91f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87a68119b0c04194a567428d04c70c06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6c53ca327e84b7e842738ea1ceb46f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}